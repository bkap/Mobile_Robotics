\documentclass{article}
\author{William Kulp, Connor Balin, and Wesley Gould of team Delta}
\title{EECS 476 Final Report}
\date{2 May 2011}
\usepackage{fullpage}
\usepackage{url}
\usepackage{listings}
\begin{document}
\maketitle

\small
\section{Architecture Description}

\subsection{Goal Publisher}

The goal publisher's sole purpose is to publish goalPose.
It does not subscribe to anything, and at present the goal pose never changes.
Also, at present, even though the Path Planner listens to the Goal Publisher, it does not take its output into consideration except for deciding where on the path to stop.
It is hoped that in the future we will implement a path finding algorithm, rather than hard coding an initial path and inserting a swerve when we detect an obstacle.
When that happens, Goal Publisher's information will become more relevant.


\subsection{LIDAR Driver}

The LIDAR Driver was provided for us by the TA's.
It pulls LIDAR pings from the LIDAR sensor at a frequency of 75hz, transforms them into map coordinates and sends them to the LIDAR Mapper.
When it was given to us, it initially used the odom frame, but we decided that we would consistently use the map frame for all of our other nodes, so that we would be working in a consistent frame which could be debugged easily by looking at the map in stage.


\subsection{LIDAR Mapper}

The LIDARmapper subscribes to LIDARCloud and publishes LIDARMap, the fattened CSpace map.
The CSPace is returned as an OccupancyGrid with an origin at -10,-10 and height and width 35 in the map frame to contain the entire area of interest for this assignment.
The points in the point cloud are fattened by ORing a pre-generated template containing a disk of radius 15cm with the pixel containing the ping and its' neighbors.
Only pings within 10m of the robot are considered and only points for which the entirety of the fattened area would lie within the grid are added to the map.
These constraints are imposed to improve the accuracy of the CSpace map, and to prevent overflowing the array respectively.
For future improvements, we are considering methods of clearing places on the CSpace map which are no longer occupied.
In general the procedure for clearing old points would be to assume that any point between the LIDAR sensor and a nearby ping (within 10 m) is now clear because it would be impossible for an object to occupy a space near the LIDAR sensor and not get hit by a ping.


\subsection{Camera Calibration}

The camera calibration node is run separately from the rest of the program.  It subscribes to the front camera and the LIDAR.  This node's goal is to find a number of points of correspondance between the camera and LIDAR.  It runs a LIDAR callback and a camera callback simultaneously to find these points.

First, in the LIDAR callback, the program searches for a rectangular feature that looks like the calibration rod.  It uses criteria of minimum separation from surroundings, minimum and maximum width, and maximum slope to discriminate the rod from the LIDAR scans.  If the rod is found, its location is recorded.

Second, in the camera callback, the program searches for an orange blob that looks like the base of the calibration rod.  The camera callback normalizes colors and uses the cvBlob library to search for orange blobs.  If the camera callback successfully finds an orange blob, it checks whether the LIDAR callback found the rod on its last iteration.  If the camera callback and the LIDAR callback both simultaneously found points, those points are stored as a correspondance.

The node runs until 120 correspondences between LIDAR and the camera are found.  Camera calibration then uses the OpenCV function findHomography to get a transformation matrix between camera coordinates and base link coordinates.  This matrix is saved to file for use in other nodes. 

\subsection{Camera}

The camera node subscribes to the feed from Jinx's front camera.  It publishes a cloud of orange points in map coordinates.  On startup, the node loads the transformation matrix that was previously calibrated by the camera calibration node.  The main action happens whenever the node receives a new image from the camera.  First, it normalizes the colors and applies thresholds to create a binary matrix containing only points that satisfy the criteria for orange.  Next, the OpenCV function findContours is called.  This reduces orange blobs to their outlines, drastically reducing the number of points returned.
Finally, the point cloud is transformed to map coordinates and published for use in the mapper node.



\subsection{Path Planner}

The path planner subscribes to LIDARMap,PoseDes and goalPose and publishes pathList.
The planner has been pre-programmed for this demo with a path that it should be following unless it sees an obstacle with an A* search on the map hopefully forthcoming over break.
At present, the response to an obstruction in the path is a hardcoded pattern consisting of a swerve left followed by a swerve right once the path to the right is clear.
The swerve is always a fixed distance, and no further attempt to find obstacles is made during curves or swerves, so the current algorithm would likely crash if there were multiple nearby obstacles or if there were an obstacle in a curve or swerve.
In general, the Speed Profiler is still attmepting to search the current path for obstacles, so the robot should not hit any obstacles, but it may get stuck.
After generating the path, the Path Planner publishes the path as a PathList.
One possible algorithm to be considered was published by Valve, a video game company, in a whitepaper, \url{http://www.valvesoftware.com/publications/2009/ai_systems_of_l4d_mike_booth.pdf}.
In general the algorithm could work quite well, but there are concerns about the re-pathing speed.
Also, we will need to impose a boundry around the world so that the pathfinding algorithm will run in finite time.

\subsection{Desired Path Crawler}

The Desired Path Crawler listens to the current speed put out by the Speed Profiler, and the PathList published by the Path Planner.
The Crawler moves a breadcrumb along the path published by the Path Planner at the speed that the Speed Profiler aledges we are moving.
It leaves the current speed blank so that the Speed Profiler can fill in what speed it would like to go.
Each type of path segment is a special case.
Lines are the easiest, the reference point given is the start point, the heading does not vary throughout the segment, and the position can just be incremented by the velocity vector each iteration.
Turning in place is also easy because the linear velocity is zero and the reference point is the point that the robot stops at, and distance is measured in terms of the number of radians turned.
Curves are the most difficult. 
For curves, the profiler needs to compute a position that moves along the curve. 
The heading is incremented or decremented by the angular velocity of the robot, and basic trigonometry is then used to find the current point along the curve.
After deciding a desired pose, the Desired Path Crawler publishes the desied path.


\subsection{Speed Profiler}

The Speed Profiler subscribes to the pose put out by the Desired Path Crawler, CSpace map published by the LIDAR Mapper, as well as the Path List put out by the Path Planner.
The Speed Profiler just fills in the linear speed at each point and republishes it with a different name, Speed Nominal.
It attempts to choose accelerate at the maximum allowed rate until it matches speed with what the Path Planner recommends in its PathList.
It is also looking to see if there are any obstacles in the current path, and will attempt to brake at the maximum safe rate to try to prevent immediate collisions.
The path planner is then expected to repath around obstacles.


\subsection{Steering Module}

The Steering Module subscribes to NominalSpeed and odom and publishes $cmd_vel$.
From NominalSpeed, the desired speed and pose are extracted for comparison with the actual pose from odom to generate steering corrections to return the robot to the desired path and schedule.
The corrections are performed by projecting the vector from desired pose to actual pose onto both the desired heading and a leftward normal to the desired heading to calculate following and lateral errors respectively.
This calculation is the same for both arcs and line segments, so the allowable path errors for good operation are less on an arc than for a line segment.
The steering correction on the arcs additionally feeds-forward the linear voloctiy correction to the angular velocity corrretion.
The corrections above nominal velocity are capped to .3 m / s for linear velocity and .5 rad / s for angular velocity and the total commanded linear velocity is constrained to be non-negative.

\subsection{Other Code Files}
\subsubsection{CSpaceFuncs}
The CSpaceFuncs file contains two functions for the manipulation of CSpace maps.  The fist, GetMap, generates an openCV matrix from an Occupancy Grid.  The other function PlotMap plots points to the map in RViz.  We didn't quite get PlotMap working yet, but we think that it could be a nice debug tool.


\section{Discussion}
\subsection{Observation of Performance}

We demonstrated that we could successfully navigate down the hallway with obstacles found using LIDAR.  
The robot was able to find the path entirely on its own, but it shaked and shuddered for reasons which will be discussed in the Bug Report.
Also, the planner node will die if there is no path, which will result in the other nodes driving the last valid path that planner put out before it died.  
The old path will likely not know about all of the obstacles in its way, so the robot will drive into things in this case, and should be stopped immediately on the death of the planner node.
We believe that it can also find obstacles using the cameras, but we only observed this behavior on a bag file and not on the actual robot.
There was not enough time to do test the camera's functionality before the final demo, when tested on the bag file, the camera node appeared to send coordinates to the mapper node in a way that resulted in a plausible CSpace map.
Furthermore, we know that the planner node can find paths successfully, therefore it is plausible to believe that we had the vision portion of the demo working as well.


\subsection{Bug Report}

There is a not well understood error in the interaction of the planner and the path crawler which only occurs when we attempt to repath on a curve or turn in place. 
The bug seemed to prevent the path from being followed at all.
Luckily for us, the bug did not appear to manifest with straight lines, so we just drove all of our paths with no turns and allowed the steering node to make up the difference.
This did not work terribly well because the steering node does not know how to steer correctly when the heading error is nearly perpindicular, and resulted in the shuddering and shaking which was observed when we ran the demo in the simulator and on an actual robot.

The planner node had an additional bug which could have been fixed with some work, but usually wasn't an issue.  
In the case that no path exists, planner will crash because it attempts to read from a priority_queue which has no elements.
The simplest fix would be to raise the WALL_THRESHOLD value, which would then allow the planner to invent a path even if it went through a known wall.  
This fix is obviously sub-optimal because planner would deliberately make paths through walls.
The failure mode for the death of planner is also an issue because the surviving nodes will attempt to follow the last known path, which almost certainly results in collisions.
Overall, this wasn't an issue because the hallway was wide enough in the demo, and we had an emergency stop button to kill the power to the motors.


The mapper node also sometimes died on initialization, but only some of the time, and the cause was never fully investigated because more than likely, just restarting all of the nodes would make it run just fine.

\subsection{Group Policies and Ideas for Improvement}

In general, our group has found it very useful to make use of version control systems and online backup, in particular, github.
We decided as a group that all positions will be in the map frame with the same origin across all nodes to reduce possible transform erros.
In addition, we found that OpenCV is rather convienent to use, and it is our opinion that we should replace all of the CSpace message types with OpenCV matrices or sparse matrices due to their convienence of use.
We have noted though that the OpenCV documentation is not accurate for the version included in CTurtle, so it is our recommendation that the class be switched over to Diamondback at the soonest possible time because the documentation appears to be much more accurate for that version.

\end{document}

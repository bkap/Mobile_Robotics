\documentclass{article}
\author{William Kulp, Connor Balin, and Wesley Gould of team Delta}
\title{EECS 476 Final Report}
\date{2 May 2011}
\usepackage{fullpage}
\usepackage{url}
\usepackage{listings}
\begin{document}
\maketitle

\small
\section{Architecture Description}

\subsection{Goal Publisher}

The goal publisher's sole purpose is to publish goalPose.
It is defunct now.
In short, we don't use it.
Its functionality was merged into planner.


\subsection{LIDAR Driver}

The LIDAR Driver was provided for us by the TA's.
It pulls LIDAR pings from the LIDAR sensor at a frequency of 75hz, transforms them into map coordinates and sends them to the LIDAR Mapper.
When it was given to us, it initially used the odom frame, but we decided that we would consistently use the map frame for all of our other nodes, so that we would be working in a consistent frame which could be debugged easily by looking at the map in stage.

It was changed very little since the midterm. The only changes to it are changes in indentation and minor bugfixes.


\subsection{Mapper}

The mapper subscribes to LIDAR_Cloud, Camera_Cloud,camera_view and odom and publishes CSPaceMap, map_cam, and map_lid.  
map_cam and map_lid, the individual sensor grids, are published solely for debug purposed and are subscribed to only by rviz.
The camera_view topic was created because it made more sense for the camera to tell the mapper what it could see than it did for the mapper to calculate it.
The values in cells of the grid correspond to the available sensor information: 0 indicates no information, while positive numbers indicate likely obstacles and negative numbers indicate likely free space.
Internally, the camera and lidar occupancies are maintained on separate grids to prevent conflicts when only one sensor can detect an obstacle.
Clearing of the camera grid is performed for the entire viewable area assuming a trapezoidal view calculated from the robot location and camera calibration
LIDAR clearing is performed via raytracing from the current robot location to the pings.
To prevent improper projection of tall objects onto the ground plane by camera, only camera points which lie within the area most recently swept out by LIDAR pings are incorporated into the grid, since others are quite likely not on the ground.
The mapper has two modes of operation with regard to occupancy calculation and a flag set in the file switches between them.
We did not demo the additive mode, but it has been tested thoroughly.  
In the additive mode, each fattened ping increments the appropriate cells on the sensor grid by the FILL_RATE, with a gaussian decay outward to the fattening radius.  
When the ADDITIVE flag is not set, a patch instead assigns the max of its value and the present occupancy at each appropriate cell.  
This approach provides a few advantages over the additive approach.  
The primary advanage is that setting rather than incrementing guarantees that the grid will not saturate and that there will be a smooth gaussian decay with distance from obstacles.  
This allows the path planner to provide paths that preferentially steer away from obstacles rather than merely not intersecting and also allows smoother performance when adjusting the parameter in planner which specifies the maximum traversable occupancy value.  
However, strictly setting the occupancy results in much greater sensitivity to spurious sensor readings than incrementing, which mitigates the effect of a single outlier.

A potential improvement to the current implementation would be to use incremental filling with a solid disk rather than a gaussian one and apply a gaussian blur to the merged occupancy grid before publishing.  
This may permit the robustness benefits of both the incremental filling and clearing and the planning benefits of the setting, but might be too expensive with the gaussian blur of the entire grid calculated at the publishing rate rather than once for the precomputed patch.

\subsection{Camera Calibration}
calibration subscribes to LIDAR and the camera
The camera calibration procedure uses identified correspondances between the LIDAR data and camera data to compute several transformation matrices.
The primary transformation calculated (and the only one used elsewhere) is the homography between the image plane and base_link coordinate system.
Additional transformations include the plan-view transform using view image specifications (physical pixel size and image physical extents) specified as parameters to the calibration procedure and a transform from the birdseye image plane to base_link co-ordinates

Use of openCV's homography matrices and associated transform functions obviates the need for use of homogeneous co-ordinates, which are handled internally, and 	implements RANSAC in the determination of the transforms.

potential correspondences are identified in the LIDAR by searching for closely grouped pings, which differ from each other by no more than a few centimeters and from pings to either side by no less than .4 meters.  Any group meeting these criteria is marked as a potential rod.
Potential correspondences are identified in the images by identifying centroids of orange blobs.  Any such points are recorded.
If, in the camera callback, both the most recent lidar scan and image show potential correspondences, the correspondence is confirmed and each point is pushed into its respective vector of correspondences.  This is done in the camera callback to guarantee that the image is fresh, which means that the robot has not travelled substantially between the corresponding sensor readings, since that would mess up the transform.
When sufficient correspondences are recorded (currently a threshold of 120), the findHomography function from opencv is called to find the homography between the image plane and base_link co-ordinates.

\subsection{Camera}

Camera subsribes to the rectified image stream and published Camera_cloud and camera_view.
Camera_Cloud is a PointCloud in map frame which contains all pixels corresponding to borders of orange blobs identified by the camera
camera_view is a PointCloud in map frame which contains points corresponding to the 4 corners of the currently viewable camera frame for use in mapper.
On initialization, the node reads from the disk the homography matrix stored by camera calibration.  This matrix is the homography between the base_link co-ordinate system and the camera's pixel co-ordinate system.  It is used for determination of map location of desired pixels, including orange blobs and image corners.
The node spins until receiving an image callback.  When a new image is recieved, the node normalizes and thresholds the colors to binarize the image and preserve only orange blobs.  Then, the contours are isolated and transformed into map co-ordinates for publishing.  Blob contours are used instead of centroids because the contour will carry accurate information about object extents even for extended and large objects.  Also, when the mapper masks non ground-plane camera points, certain extended objects not on lidar could disappear entirely if represented by a centroid.

\subsection{Path Planner}

The path planner subscribes to LIDARMap,PoseDes and goalPose and publishes pathList.  
The LIDARMap is just an occupancy grid representing the likelyhood of there being a wall in a given location where 0 is unknown, CHAR_MIN is an open area, and CHAR_MAX is a wall.

We implemented an A* search in PlannerFuncs which took the CSpace map published by Mapper and attempted to find a path on it.
The standard implementation of A* search keeps a hashSet of "expanded nodes" which have already been explored by the search algorithm, and checks membership based purely on the state variables of each Node.  
Because we are constrained to a 2D space, it is simple enough to keep a 2D grid of pointers to expanded nodes, or rather a Node***. 
This pointers start being initialized to NULL, and are initialized to values if and only if those states are added to the expanded set.
This gives us a trivial hash set implementation with a very fast hash function and is guaranteed not to result in collisions.

In addition, the standard implementation keeps a priority queue of nodes, sorted by the sum of path cost and heuristic with the lowest values being picked first.
In our implementation, we just use a standard C++ priority queue, but by default, it is a max heap using the less than operator.
Since we needed to define the operator already, we simply used greater than instead of less than inside of the definition of the operator to make the max heap into a min heap.

The Path Planner then makes a call to the approxPolyDP function in openCV to reduce the number of perpendicular paths, which is necessary even though it could lead to collisions.
The importance of this is discussed in the bug report section of the report.

To update the path, the planner follows the rule that it only changes the future paths, and not the current path or previous paths. 
This is done because the desired path crawler uses both the segment number and the position in the vector of paths to lookup information about the segment that it is on.  Because the crawler was unable to repath successfully with anything other than a straight line, we made a last minute change to have the path planner only make straight lines.


\subsection{Desired Path Crawler}

The Desired Path Crawler listens to the current speed put out by the Speed Profiler, and the PathList published by the Path Planner.
The Crawler moves a breadcrumb along the path published by the Path Planner at the speed that the Speed Profiler alleges we are moving.
It leaves the current speed blank so that the Speed Profiler can fill in what speed it would like to go.
Each type of path segment is a special case.
Lines are the easiest, the reference point given is the start point, the heading does not vary throughout the segment, and the position can just be incremented by the velocity vector each iteration.
Turning in place is also easy because the linear velocity is zero and the reference point is the point that the robot stops at, and distance is measured in terms of the number of radians turned.
Curves are the most difficult. 
For curves, the profiler needs to compute a position that moves along the curve. 
The heading is incremented or decremented by the angular velocity of the robot, and basic trigonometry is then used to find the current point along the curve.
After deciding a desired pose, the Desired Path Crawler publishes the desired path.

It was changed very little since the midterm. The only changes to it are changes in indentation and minor bugfixes.


\subsection{Speed Profiler}

The Speed Profiler subscribes to the pose put out by the Desired Path Crawler, CSpace map published by the LIDAR Mapper, as well as the Path List put out by the Path Planner.
The Speed Profiler just fills in the linear speed at each point and republishes it with a different name, Speed Nominal.
It attempts to choose accelerate at the maximum allowed rate until it matches speed with what the Path Planner recommends in its PathList.
It is also looking to see if there are any obstacles in the current path, and will attempt to brake at the maximum safe rate to try to prevent immediate collisions.
The path planner is then expected to repath around obstacles.

It was changed very little since the midterm. The only changes to it are changes in indentation and minor bugfixes.

\subsection{Steering Module}

The Steering Module subscribes to NominalSpeed and odom and publishes $cmd_vel$.
From NominalSpeed, the desired speed and pose are extracted for comparison with the actual pose from odom to generate steering corrections to return the robot to the desired path and schedule.
The corrections are performed by projecting the vector from desired pose to actual pose onto both the desired heading and a leftward normal to the desired heading to calculate following and lateral errors respectively.
This calculation is the same for both arcs and line segments, so the allowable path errors for good operation are less on an arc than for a line segment.
The steering correction on the arcs additionally feeds-forward the linear velocity correction to the angular velocity correction.
The corrections above nominal velocity are capped to .3 m / s for linear velocity and .5 rad / s for angular velocity and the total commanded linear velocity is constrained to be non-negative.

It was changed very little since the midterm. The only changes to it are changes in indentation and minor bugfixes.

\subsection{Other Code Files}
\subsubsection{CSpaceFuncs}
The CSpaceFuncs file contains two functions for the manipulation of CSpace maps.  The fist, GetMap, generates an openCV matrix from an Occupancy Grid.  The other function PlotMap plots points to the map in RViz.  We didn't quite get PlotMap working yet, but we think that it could be a nice debug tool.


\section{Discussion}
\subsection{Observation of Performance}

We demonstrated that we could successfully navigate down the hallway with obstacles found using LIDAR.  
The robot was able to find the path entirely on its own, but it shaked and shuddered for reasons which will be discussed in the Bug Report.
Also, the planner node will die if there is no path, which will result in the other nodes driving the last valid path that planner put out before it died.  
The old path will likely not know about all of the obstacles in its way, so the robot will drive into things in this case, and should be stopped immediately on the death of the planner node.
We believe that it can also find obstacles using the cameras, but we only observed this behavior on a bag file and not on the actual robot.
There was not enough time to do test the camera's functionality before the final demo, when tested on the bag file, the camera node appeared to send coordinates to the mapper node in a way that resulted in a plausible CSpace map.
Furthermore, we know that the planner node can find paths successfully, therefore it is plausible to believe that we had the vision portion of the demo working as well.


\subsection{Bug Report}

There is a not well understood error in the interaction of the planner and the path crawler which only occurs when we attempt to repath on a curve or turn in place. 
The bug seemed to prevent the path from being followed at all.
Luckily for us, the bug did not appear to manifest with straight lines, so we just drove all of our paths with no turns and allowed the steering node to make up the difference.
This did not work terribly well because the steering node does not know how to steer correctly when the heading error is nearly perpendicular, and resulted in the shuddering and shaking which was observed when we ran the demo in the simulator and on an actual robot.

The planner node had an additional bug which could have been fixed with some work, but usually wasn't an issue.  
In the case that no path exists, planner will crash because it attempts to read from a priority_queue which has no elements.
The simplest fix would be to raise the WALL_THRESHOLD value, which would then allow the planner to invent a path even if it went through a known wall.  
This fix is obviously sub-optimal because planner would deliberately make paths through walls.
The failure mode for the death of planner is also an issue because the surviving nodes will attempt to follow the last known path, which almost certainly results in the robot becoming stop when profiler emergency stops and no replanning occurs.  Luckily this did not happen much.
Intermittently, the mapper dies on initialization.  Since it never dies during operation, it is likely that it fails to wait on something necessary, but we never looked into it since restarting the node fixed it and it ran stably once it initialized.

\subsection{Group Policies and Ideas for Improvement}

In general, our group has found it very useful to make use of version control systems and online backup, in particular, github.
We decided as a group that all positions will be in the map frame with the same origin across all nodes to reduce possible transform errors.
In addition, we found that OpenCV is rather convenient to use, and it is our opinion that we should replace all of the CSpace message types with OpenCV matrices or sparse matrices due to their convenience of use.
We have noted though that the OpenCV documentation is not accurate for the version included in CTurtle, so it is our recommendation that the class be switched over to Diamondback at the soonest possible time because the documentation appears to be much more accurate for that version.

\end{document}
